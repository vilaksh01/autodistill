{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>notebooks | inference | autodistill | collect</p> <p> </p> <p>Autodistill uses big, slower foundation models to train small, faster supervised models. Using <code>autodistill</code>, you can go from unlabeled images to inference on a custom model running at the edge with no human intervention in between.</p> <p> </p> <p>Currently, <code>autodistill</code> supports vision tasks like object detection and instance segmentation, but in the future it can be expanded to support language (and other) models.</p>"},{"location":"#quicklinks","title":"\ud83d\udd17 Quicklinks","text":"Tutorial Docs Supported Models Contribute"},{"location":"#example-output","title":"\ud83d\udc40 Example Output","text":"<p>Here are example predictions of a Target Model detecting milk bottles and bottlecaps after being trained on an auto-labeled dataset using Autodistill (see the Autodistill YouTube video for a full walkthrough):</p> <p> </p>"},{"location":"#features","title":"\ud83d\ude80 Features","text":"<ul> <li>\ud83d\udd0c Pluggable interface to connect models together</li> <li>\ud83e\udd16 Automatically label datasets</li> <li>\ud83d\udc30 Train fast supervised models</li> <li>\ud83d\udd12 Own your model</li> <li>\ud83d\ude80 Deploy distilled models to the cloud or the edge</li> </ul>"},{"location":"#basic-concepts","title":"\ud83d\udcda Basic Concepts","text":"<p>To use <code>autodistill</code>, you input unlabeled data into a Base Model which uses an Ontology to label a Dataset that is used to train a Target Model which outputs a Distilled Model fine-tuned to perform a specific Task.</p> <p> </p> <p>Autodistill defines several basic primitives:</p> <ul> <li>Task - A Task defines what a Target Model will predict. The Task for each component (Base Model, Ontology, and Target Model) of an <code>autodistill</code> pipeline must match for them to be compatible with each other. Object Detection and Instance Segmentation are currently supported through the <code>detection</code> task. <code>classification</code> support will be added soon.</li> <li>Base Model - A Base Model is a large foundation model that knows a lot about a lot. Base models are often multimodal and can perform many tasks. They're large, slow, and expensive. Examples of Base Models are GroundedSAM and GPT-4's upcoming multimodal variant. We use a Base Model (along with unlabeled input data and an Ontology) to create a Dataset.</li> <li>Ontology - an Ontology defines how your Base Model is prompted, what your Dataset will describe, and what your Target Model will predict. A simple Ontology is the <code>CaptionOntology</code> which prompts a Base Model with text captions and maps them to class names. Other Ontologies may, for instance, use a CLIP vector or example images instead of a text caption.</li> <li>Dataset - a Dataset is a set of auto-labeled data that can be used to train a Target Model. It is the output generated by a Base Model.</li> <li>Target Model - a Target Model is a supervised model that consumes a Dataset and outputs a distilled model that is ready for deployment. Target Models are usually small, fast, and fine-tuned to perform a specific task very well (but they don't generalize well beyond the information described in their Dataset). Examples of Target Models are YOLOv8 and DETR.</li> <li>Distilled Model - a Distilled Model is the final output of the <code>autodistill</code> process; it's a set of weights fine-tuned for your task that can be deployed to get predictions.</li> </ul>"},{"location":"#theory-and-limitations","title":"\ud83d\udca1 Theory and Limitations","text":"<p>Human labeling is one of the biggest barriers to broad adoption of computer vision. It can take thousands of hours to craft a dataset suitable for training a production model. The process of distillation for training supervised models is not new, in fact, traditional human labeling is just another form of distillation from an extremely capable Base Model (the human brain \ud83e\udde0).</p> <p>Foundation models know a lot about a lot, but for production we need models that know a lot about a little.</p> <p>As foundation models get better and better they will increasingly be able to augment or replace humans in the labeling process. We need tools for steering, utilizing, and comparing these models. Additionally, these foundation models are big, expensive, and often gated behind private APIs. For many production use-cases, we need models that can run cheaply and in realtime at the edge.</p> <p> </p> <p>Autodistill's Base Models can already create datasets for many common use-cases (and through creative prompting and few-shotting we can expand their utility to many more), but they're not perfect yet. There's still a lot of work to do; this is just the beginning and we'd love your help testing and expanding the capabilities of the system!</p>"},{"location":"#installation","title":"\ud83d\udcbf Installation","text":"<p>Autodistill is modular. You'll need to install the <code>autodistill</code> package (which defines the interfaces for the above concepts) along with Base Model and Target Model plugins (which implement specific models).</p> <p>By packaging these separately as plugins, dependency and licensing incompatibilities are minimized and new models can be implemented and maintained by anyone.</p> <p>Example:  <pre><code>pip install autodistill autodistill-grounded-sam autodistill-yolov8\n</code></pre></p> Install from source  You can also clone the project from GitHub for local development:  <pre><code>git clone https://github.com/roboflow/autodistill\ncd autodistill\npip install -e .\n</code></pre> <p>Additional Base and Target models are enumerated below.</p>"},{"location":"#quickstart","title":"\ud83d\ude80 Quickstart","text":"<p>See the demo Notebook for a quick introduction to <code>autodistill</code>. This notebook walks through building a milk container detection model with no labeling.</p> <p>Below, we have condensed key parts of the notebook for a quick introduction to <code>autodistill</code>.</p> <p>You can also run Autodistill in one command. First, install <code>autodistill</code>:</p> <pre><code>pip install autodistill\n</code></pre> <p>Then, run:</p> <pre><code>autodistill images --base=\"grounding_dino\" --target=\"yolov8\" --ontology '{\"prompt\": \"label\"}' --output=\"./dataset\"\n</code></pre> <p>This command will label all images in a directory called <code>images</code> with Grounding DINO and use the labeled images to train a YOLOv8 model. Grounding DINO will label all images with the \"prompt\" and save the label as the \"label\". You can specify as many prompts and labels as you want. The resulting dataset will be saved in a folder called <code>dataset</code>.</p>"},{"location":"#install-packages","title":"Install Packages","text":"<p>For this example, we'll show how to distill GroundedSAM into a small YOLOv8 model using autodistill-grounded-sam and autodistill-yolov8.</p> <pre><code>pip install autodistill autodistill-grounded-sam autodistill-yolov8\n</code></pre>"},{"location":"#distill-a-model","title":"Distill a Model","text":"<pre><code>from autodistill_grounded_sam import GroundedSAM\nfrom autodistill.detection import CaptionOntology\nfrom autodistill_yolov8 import YOLOv8\n\n# define an ontology to map class names to our GroundingDINO prompt\n# the ontology dictionary has the format {caption: class}\n# where caption is the prompt sent to the base model, and class is the label that will\n# be saved for that caption in the generated annotations\nbase_model = GroundedSAM(ontology=CaptionOntology({\"shipping container\": \"container\"}))\n\n# label all images in a folder called `context_images`\nbase_model.label(\n  input_folder=\"./images\",\n  output_folder=\"./dataset\"\n)\n\ntarget_model = YOLOv8(\"yolov8n.pt\")\ntarget_model.train(\"./dataset/data.yaml\", epochs=200)\n\n# run inference on the new model\npred = target_model.predict(\"./dataset/valid/your-image.jpg\", confidence=0.5)\nprint(pred)\n\n# optional: upload your model to Roboflow for deployment\nfrom roboflow import Roboflow\n\nrf = Roboflow(api_key=\"API_KEY\")\nproject = rf.workspace().project(\"PROJECT_ID\")\nproject.version(DATASET_VERSION).deploy(model_type=\"yolov8\", model_path=f\"./runs/detect/train/\")\n</code></pre> Visualize Predictions  To plot the annotations for a single image using `autodistill`, you can use the code below. This code is helpful to visualize the annotations generated by your base model (i.e. GroundedSAM) and the results from your target model (i.e. YOLOv8).  <pre><code>import supervision as sv\nimport cv2\n\nimg_path = \"./images/your-image.jpeg\"\n\nimage = cv2.imread(img_path)\n\ndetections = base_model.predict(img_path)\n# annotate image with detections\nbox_annotator = sv.BoxAnnotator()\n\nlabels = [\n    f\"{base_model.ontology.classes()[class_id]} {confidence:0.2f}\"\n    for _, _, confidence, class_id, _ in detections\n]\n\nannotated_frame = box_annotator.annotate(\n    scene=image.copy(), detections=detections, labels=labels\n)\n\nsv.plot_image(annotated_frame, (16, 16))\n</code></pre>"},{"location":"#available-models","title":"\ud83d\udccd Available Models","text":"<p>Our goal is for <code>autodistill</code> to support using all foundation models as Base Models and most SOTA supervised models as Target Models. We focused on object detection and segmentation tasks first but plan to launch classification support soon! In the future, we hope <code>autodistill</code> will also be used for models beyond computer vision.</p> <ul> <li>\u2705 - complete (click row/column header to go to repo)</li> <li>\ud83d\udea7 - work in progress</li> </ul>"},{"location":"#object-detection","title":"object detection","text":"base / target YOLOv8 YOLO-NAS YOLOv5 DETR YOLOv6 YOLOv7 MT-YOLOv6 DETIC \u2705 \u2705 \u2705 \u2705 \ud83d\udea7 GroundedSAM \u2705 \u2705 \u2705 \u2705 \ud83d\udea7 GroundingDINO \u2705 \u2705 \u2705 \u2705 \ud83d\udea7 OWL-ViT \u2705 \u2705 \u2705 \u2705 \ud83d\udea7 SAM-CLIP \u2705 \u2705 \u2705 \u2705 \ud83d\udea7 LLaVA-1.5 \u2705 \u2705 \u2705 \u2705 \ud83d\udea7 Kosmos-2 \u2705 \u2705 \u2705 \u2705 \ud83d\udea7 OWLv2 \u2705 \u2705 \u2705 \u2705 \ud83d\udea7 Roboflow Universe Models (50k+ pre-trained models) \u2705 \u2705 \u2705 \u2705 \ud83d\udea7 Azure Custom Vision \u2705 \u2705 \u2705 \u2705 \ud83d\udea7 AWS Rekognition \u2705 \u2705 \u2705 \u2705 \ud83d\udea7 Google Vision \u2705 \u2705 \u2705 \u2705 \ud83d\udea7"},{"location":"#instance-segmentation","title":"instance segmentation","text":"base / target YOLOv8 YOLO-NAS YOLOv5 YOLOv7 Segformer GroundedSAM \u2705 \ud83d\udea7 \ud83d\udea7 SAM-CLIP \u2705 \ud83d\udea7 \ud83d\udea7 SegGPT \u2705 \ud83d\udea7 \ud83d\udea7 FastSAM \ud83d\udea7 \ud83d\udea7 \ud83d\udea7"},{"location":"#classification","title":"classification","text":"base / target ViT YOLOv8 YOLOv5 CLIP \u2705 \u2705 \ud83d\udea7 MetaCLIP \u2705 \u2705 \ud83d\udea7 DINOv2 \u2705 \u2705 \ud83d\udea7 BLIP \u2705 \u2705 \ud83d\udea7 ALBEF \u2705 \u2705 \ud83d\udea7 FastViT \u2705 \u2705 \ud83d\udea7 Fuyu \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 Open Flamingo \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 GPT-4 PaLM-2"},{"location":"#roboflow-model-deployment-support","title":"Roboflow Model Deployment Support","text":"<p>You can optionally deploy some Target Models trained using Autodistill on Roboflow. Deploying on Roboflow allows you to use a range of concise SDKs for using your model on the edge, from roboflow.js for web deployment to NVIDIA Jetson devices.</p> <p>The following Autodistill Target Models are supported by Roboflow for deployment:</p> model name Supported? YOLOv8 Object Detection \u2705 YOLOv8 Instance Segmentation \u2705 YOLOv5 Object Detection \u2705 YOLOv5 Instance Segmentation \u2705 YOLOv8 Classification"},{"location":"#video-guides","title":"\ud83c\udfac Video Guides","text":"<p> Autodistill: Train YOLOv8 with ZERO Annotations Published: 8 June 2023 In this video, we will show you how to use a new library to train a YOLOv8 model to detect bottles moving on a conveyor line. Yes, that's right - zero annotation hours are required! We dive deep into Autodistill's functionality, covering topics from setting up your Python environment and preparing your images, to the thrilling automatic annotation of images. </p>"},{"location":"#community-resources","title":"\ud83d\udca1 Community Resources","text":"<ul> <li>Distill Large Vision Models into Smaller, Efficient Models with Autodistill: Announcement post with written guide on how to use Autodistill</li> <li>Comparing AI-Labeled Data to Human-Labeled Data: A qualitative evaluation of Grounding DINO used with Autodistill across various tasks and domains.</li> <li>How to Evaluate Autodistill Prompts with CVevals: Evaluate Autodistill prompts.</li> <li>Autodistill: Label and Train a Computer Vision Model in Under 20 Minutes: Building a model to detect planes in under 20 minutes.</li> <li>Comparing AI-Labeled Data to Human-Labeled Data: Explore the strengths and limitations of a base model used with Autoditsill.</li> <li>Train an Image Classification Model with No Labeling: Use Grounded SAM to automatically label images for training an Ultralytics YOLOv8 classification model.</li> <li>Train a Segmentation Model with No Labeling: Use CLIP to automatically label images for training an Ultralytics YOLOv8 segmentation model.</li> <li>File a PR to add your own resources here!</li> </ul>"},{"location":"#roadmap","title":"\ud83d\uddfa\ufe0f Roadmap","text":"<p>Apart from adding new models, there are several areas we plan to explore with <code>autodistill</code> including:</p> <ul> <li>\ud83d\udca1 Ontology creation &amp; prompt engineering</li> <li>\ud83d\udc69\u200d\ud83d\udcbb Human in the loop support</li> <li>\ud83e\udd14 Model evaluation</li> <li>\ud83d\udd04 Active learning</li> <li>\ud83d\udcac Language tasks</li> </ul>"},{"location":"#contributing","title":"\ud83c\udfc6 Contributing","text":"<p>We love your input! Please see our contributing guide to get started. Thank you \ud83d\ude4f to all our contributors!</p>"},{"location":"#license","title":"\ud83d\udc69\u200d\u2696\ufe0f License","text":"<p>The <code>autodistill</code> package is licensed under an Apache 2.0. Each Base or Target model plugin may use its own license corresponding with the license of its underlying model. Please refer to the license in each plugin repo for more information.</p>"},{"location":"#frequently-asked-questions","title":"Frequently Asked Questions \u2753","text":""},{"location":"#what-causes-the-pytorchstreamreader-failed-reading-zip-archive-failed-finding-central-directory-error","title":"What causes the <code>PytorchStreamReader failed reading zip archive: failed finding central directory</code> error?","text":"<p>This error is caused when PyTorch cannot load the model weights for a model. Go into the <code>~/.cache/autodistill</code> directory and delete the folder associated with the model you are trying to load. Then, run your code again. The model weights will be downloaded from scratch. Leave the installation process uninterrupted.</p>"},{"location":"#explore-more-roboflow-open-source-projects","title":"\ud83d\udcbb explore more Roboflow open source projects","text":"Project Description supervision General-purpose utilities for use in computer vision projects, from predictions filtering and display to object tracking to model evaluation. Autodistill (this project) Automatically label images for use in training computer vision models. Inference An easy-to-use, production-ready inference server for computer vision supporting deployment of many popular model architectures and fine-tuned models. Notebooks Tutorials for computer vision tasks, from training state-of-the-art models to tracking objects to counting objects in a zone. Collect Automated, intelligent data collection powered by CLIP."},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p>"},{"location":"changelog/#010-2023-05-17","title":"[0.1.0] - 2023-05-17","text":"<p>Launched the <code>autodistill</code> package with support for GroundingSAM.</p>"},{"location":"contributing/","title":"Contributing to Autodistill","text":"<p>We welcome assistance to help us improve Autodistill! See our Contributing guidelines for more information on how to get started.</p>"},{"location":"contributing/#contributor-resources","title":"Contributor Resources","text":"<ul> <li>Base Model Template: A starter template for creating base models. This template is accompanied by documentation on how to use the template.</li> <li>Target Model Template: A target template for creating target models. This template is accompanied by documentation on how to use the template.</li> </ul>"},{"location":"base_models/clip/","title":"CLIP","text":"<p>CLIP, developed by OpenAI, is a computer vision model trained using pairs of images and text. You can use CLIP with autodistill for image classification.</p>"},{"location":"base_models/clip/#installation","title":"Installation","text":"<p>To use CLIP with autodistill, you need to install the following dependency:</p> <pre><code>pip3 install autodistill-clip\n</code></pre>"},{"location":"base_models/clip/#quickstart","title":"Quickstart","text":"<pre><code>from autodistill_clip import CLIP\nfrom autodistill.detection import CaptionOntology\n\n# define an ontology to map class names to our GroundingDINO prompt\n# the ontology dictionary has the format {caption: class}\n# where caption is the prompt sent to the base model, and class is the label that will\n# be saved for that caption in the generated annotations\n# then, load the model\nbase_model = CLIP(\n    ontology=CaptionOntology(\n        {\n            \"person\": \"person\",\n            \"a forklift\": \"forklift\"\n        }\n    )\n)\nbase_model.label(\"./context_images\", extension=\".jpeg\")\n</code></pre>"},{"location":"base_models/detic/","title":"DETIC","text":"<p>DETIC is a transformer-based object detection and segmentation model developed by Meta Research.</p>"},{"location":"base_models/detic/#installation","title":"Installation","text":"<p>To use DETIC with autodistill, you need to install the following dependency:</p> <pre><code>pip3 install autodistill-detic\n</code></pre>"},{"location":"base_models/detic/#quickstart","title":"Quickstart","text":"<pre><code>from autodistill_detic import DETIC\nfrom autodistill.detection import CaptionOntology\n\n# define an ontology to map class names to our DETIC prompt\n# the ontology dictionary has the format {caption: class}\n# where caption is the prompt sent to the base model, and class is the label that will\n# be saved for that caption in the generated annotations\n# then, load the model\nbase_model = DETIC(\n    ontology=CaptionOntology(\n        {\n            \"person\": \"person\",\n        }\n    )\n)\nbase_model.label(\"./context_images\", extension=\".jpg\")\n</code></pre>"},{"location":"base_models/fastsam/","title":"FastSAM","text":"<p>FastSAM is a segmentation model trained on 2% of the SA-1B dataset used to train the Segment Anything Model.</p> <p>Read the full Autodistill documentation.</p> <p>Read the FastSAM Autodistill documentation.</p>"},{"location":"base_models/fastsam/#installation","title":"Installation","text":"<p>To use FastSAM with autodistill, you need to install the following dependency:</p> <pre><code>pip3 install autodistill-fastsam\n</code></pre>"},{"location":"base_models/fastsam/#quickstart","title":"Quickstart","text":"<pre><code>from autodistill_fastsam import FastSAM\n\n# define an ontology to map class names to our FastSAM prompt\n# the ontology dictionary has the format {caption: class}\n# where caption is the prompt sent to the base model, and class is the label that will\n# be saved for that caption in the generated annotations\n# then, load the model\nbase_model = FastSAM(\n    ontology=CaptionOntology(\n        {\n            \"person\": \"person\",\n            \"a forklift\": \"forklift\"\n        }\n    )\n)\nbase_model.label(\"./context_images\", extension=\".jpeg\")\n</code></pre>"},{"location":"base_models/groundedsam/","title":"Grounded SAM","text":"<p>Grounded SAM uses the Segment Anything Model to identify objects in an image and assign labels to each image.</p>"},{"location":"base_models/groundedsam/#installation","title":"Installation","text":"<p>To use the Grounded SAM base model, you will need to install the following dependency:</p> <pre><code>pip3 install autodistill-grounded-sam\n</code></pre>"},{"location":"base_models/groundedsam/#quickstart","title":"Quickstart","text":"<pre><code>from autodistill_grounded_sam import GroundedSAM\nfrom autodistill.detection import CaptionOntology\n\n\n# define an ontology to map class names to our GroundingDINO prompt\n# the ontology dictionary has the format {caption: class}\n# where caption is the prompt sent to the base model, and class is the label that will\n# be saved for that caption in the generated annotations\n# then, load the model\nbase_model = GroundedSAM(ontology=CaptionOntology({\"shipping container\": \"container\"}))\n\n# label all images in a folder called `context_images`\nbase_model.label(\"./context_images\", extension=\".jpeg\")\n</code></pre>"},{"location":"base_models/grounding-dino/","title":"Grounding DINO","text":"<p>Grounding DINO is a zero-shot object detection model developed by IDEA Research. You can distill knowledge from Grounding DINO into a smaller model using Autodistill.</p>"},{"location":"base_models/grounding-dino/#installation","title":"Installation","text":"<p>To use the Grounded dino base model, you will need to install the following dependency:</p> <pre><code>pip3 install autodistill-grounding-dino\n</code></pre>"},{"location":"base_models/grounding-dino/#quickstart","title":"Quickstart","text":"<pre><code>from autodistill_grounding_dino import GroundingDINO\nfrom autodistill.detection import CaptionOntology\n\n\n# define an ontology to map class names to our GroundingDINO prompt\n# the ontology dictionary has the format {caption: class}\n# where caption is the prompt sent to the base model, and class is the label that will\n# be saved for that caption in the generated annotations\n# then, load the model\nbase_model = GroundedSAM(ontology=CaptionOntology({\"shipping container\": \"container\"}))\n\n# label all images in a folder called `context_images`\nbase_model.label(\"./context_images\", extension=\".jpeg\")\n</code></pre>"},{"location":"base_models/owlvit/","title":"OWL-ViT","text":"<p>OWL-ViT is a transformer-based object detection model developed by Google Research.</p>"},{"location":"base_models/owlvit/#installation","title":"Installation","text":"<p>To use OWL-ViT with autodistill, you need to install the following dependency:</p> <pre><code>pip3 install autodistill-owl-vit\n</code></pre>"},{"location":"base_models/owlvit/#quickstart","title":"Quickstart","text":"<pre><code>from autodistill_owl_vit import OWLViT\nfrom autodistill.detection import CaptionOntology\n\n# define an ontology to map class names to our OWLViT prompt\n# the ontology dictionary has the format {caption: class}\n# where caption is the prompt sent to the base model, and class is the label that will\n# be saved for that caption in the generated annotations\n# then, load the model\nbase_model = OWLViT(\n    ontology=CaptionOntology(\n        {\n            \"person\": \"person\",\n            \"a forklift\": \"forklift\"\n        }\n    )\n)\nbase_model.label(\"./context_images\", extension=\".jpg\")\n</code></pre>"},{"location":"base_models/sam-clip/","title":"SAM-CLIP","text":"<p>SAM-CLIP uses the Segment Anything Model to identify objects in an image and assign labels to each image. Then, CLIP is used to find masks that are related to the given prompt.</p>"},{"location":"base_models/sam-clip/#installation","title":"Installation","text":"<p>To use the SAM-CLIP base model, you will need to install the following dependency:</p> <pre><code>pip3 install autodistill-sam-clip\n</code></pre>"},{"location":"base_models/sam-clip/#quickstart","title":"Quickstart","text":"<pre><code>from autodistill_sam_clip import SAMCLIP\nfrom autodistill.detection import CaptionOntology\n\n\n# define an ontology to map class names to our CLIP prompt\n# the ontology dictionary has the format {caption: class}\n# where caption is the prompt sent to the base model, and class is the label that will\n# be saved for that caption in the generated annotations\n# then, load the model\nbase_model = SAMCLIP(ontology=CaptionOntology({\"shipping container\": \"container\"}))\n\n# label all images in a folder called `context_images`\nbase_model.label(\"./context_images\", extension=\".jpeg\")\n</code></pre>"},{"location":"base_models/seggpt/","title":"SegGPT","text":"<p>SegGPT is a transformer-based, few-shot semantic segmentation model developed by BAAI Vision.</p> <p>This model performs well on task-specific segmentation tasks when given a few labeled images from which to learn features about the objects you want to identify.</p>"},{"location":"base_models/seggpt/#installation","title":"Installation","text":"<p>To use SegGPT with Autodistill, you need to install the following dependency:</p> <pre><code>pip3 install autodistill-seggpt\n</code></pre>"},{"location":"base_models/seggpt/#about-seggpt","title":"About SegGPT","text":"<p>SegGPT performs \"in-context\" segmentation. This means it requires a handful of pre-labelled \"context\" images.</p> <p>You will need some labeled images to use SegGPT. Don't have any labeled images? Check out Roboflow Annotate, a feature-rich annotation tool from which you can export data for use with Autodistill.</p>"},{"location":"base_models/seggpt/#quickstart","title":"Quickstart","text":"<pre><code>from autodistill_seggpt import SegGPT, FewShotOntology\n\nbase_model = SegGPT(\n    ontology=FewShotOntology(supervision_dataset)\n)\n\nbase_model.label(\"./unlabelled-climbing-photos\", extension=\".jpg\")\n</code></pre>"},{"location":"base_models/seggpt/#how-to-load-data-from-roboflow","title":"How to load data from Roboflow","text":"<p>Labelling and importing images is easy!</p> <p>You can use Roboflow Annotate to label a few images (5-10 should work fine). For your Project Type, make sure to pick Instance Segmentation, as you will be labelling with polygons.</p> <p>Once you have labelled your images, you can press Generate &gt; Generate New Version. You can use all the default options--no Augmentations are necessary.</p> <p>Once your dataset version is generated, you can press Export &gt; Continue.</p> <p>Then you will get some download code to copy. It should look something like this:</p> <pre><code>!pip install roboflow\n\nfrom roboflow import Roboflow\nrf = Roboflow(api_key=\"ABCDEFG\")\nproject = rf.workspace(\"lorem-ipsum\").project(\"dolor-sit-amet\")\ndataset = project.version(1).download(\"yolov8\")\n</code></pre> <p>Note: if you are not using a notebook environment, you should remove <code>!pip install roboflow</code> from your code, and run <code>pip install roboflow</code> in your terminal instead.</p> <p>To import your dataset into Autodistill, run the following:</p> <pre><code>import supervision as sv\n\nsupervision_dataset = sv.DetectionDataset.from_yolo(\n    images_directory_path=f\"{dataset.location}/train/images\",\n    annotations_directory_path=f\"{dataset.location}/train/labels\",\n    data_yaml_path=f\"{dataset.location}/data.yaml\"\n)\n</code></pre>"},{"location":"target_models/detr/","title":"DETR","text":"<p>DETR is a transformer-based computer vision model you can use for object detection. Autodistill supports training a model using the Meta Research Resnet 50 checkpoint.</p>"},{"location":"target_models/detr/#installation","title":"Installation","text":"<p>To use DETR with autodistill, you need to install the following dependency:</p> <pre><code>pip3 install autodistill-detr\n</code></pre>"},{"location":"target_models/detr/#quickstart","title":"Quickstart","text":"<pre><code>from autodistill_detr import DETR\n\n# load the model\ntarget_model = DETR()\n\n# train for 10 epochs\ntarget_model.train(\"./roads\", epochs=10)\n\n# run inference on an image\ntarget_model.predict(\"./roads/valid/-3-_jpg.rf.bee113a09b22282980c289842aedfc4a.jpg\")\n</code></pre>"},{"location":"target_models/dinov2/","title":"DINOv2","text":"<p>CLIP is not fully supported by Autodistill. Check back later for updates.</p> <p>DINOv2, developed by Meta Research, is a self-supervised training method for computer vision models. This library uses DINOv2 image embeddings with SVM to build a classification model.</p> <p>Read the full Autodistill documentation.</p> <p>Read the DINOv2 Autodistill documentation.</p>"},{"location":"target_models/dinov2/#installation","title":"Installation","text":"<p>To use DINOv2 with autodistill, you need to install the following dependency:</p> <pre><code>pip3 install autodistill-dinov2\n</code></pre>"},{"location":"target_models/dinov2/#quickstart","title":"Quickstart","text":"<pre><code>from autodistill_dinov2 import DINOv2\n\ntarget_model = DINOv2()\n\n# train a model\n# specify the directory where your annotations (in multiclass classification folder format)\n# DINOv2 embeddings are saved in a file called \"embeddings.json\" the folder in which you are working\n# with the structure {filename: embedding}\ntarget_model.train(\"./context_images_labeled\")\n\n# run inference on the new model\npred = target_model.predict(\"./context_images_labeled/train/images/dog-7.jpg\")\n</code></pre>"},{"location":"target_models/vit/","title":"ViT","text":"<p>ViT is not fully supported by Autodistill. Check back later for updates.</p> <p>ViT is a classification model pre-trained on ImageNet-21k, developed by Google. You can train ViT classification models using Autodistill.</p>"},{"location":"target_models/vit/#installation","title":"Installation","text":"<p>To use the ViT target model, you will need to install the following dependency:</p> <pre><code>pip3 install autodistill-vit\n</code></pre>"},{"location":"target_models/vit/#quickstart","title":"Quickstart","text":"<pre><code>from autodistill_vit import ViT\n\ntarget_model = ViT()\n\n# train a model from a classification folder structure\ntarget_model.train(\"./context_images_labeled/\", epochs=200)\n\n# run inference on the new model\npred = target_model.predict(\"./context_images_labeled/train/images/dog-7.jpg\", conf=0.01)\n</code></pre>"},{"location":"target_models/yolo-nas/","title":"YOLO-NAS","text":"<p>YOLO-NAS is an object detection model developed by Deci AI.</p>"},{"location":"target_models/yolo-nas/#installation","title":"Installation","text":"<p>To use the YOLO-NAS target model, you will need to install the following dependency:</p> <pre><code>pip3 install autodistill-yolo-nas\n</code></pre>"},{"location":"target_models/yolo-nas/#quickstart","title":"Quickstart","text":"<pre><code>from autodistill_yolo_nas import YOLONAS\n\ntarget_model = YOLONAS()\n\n# train a model\n# specify the directory where your annotations (in YOLO format) are stored\ntarget_model.train(\"./context_images_labeled\", epochs=20)\n\n# run inference on the new model\npred = target_model.predict(\"./context_images_labeled/train/images/dog-7.jpg\", confidence=0.01)\n</code></pre>"},{"location":"target_models/yolov5/","title":"YOLOv5","text":"<p>YOLOv5 is an open-source computer vision model by Ultralytics, the creators of YOLOv5. You can use <code>autodistill</code> to train a YOLOv5 object detection model on a dataset of labelled images generated by the base models that <code>autodistill</code> supports.</p>"},{"location":"target_models/yolov5/#installation","title":"Installation","text":"<p>To use the YOLOv5 target model, you will need to install the following dependency:</p> <pre><code>pip3 install autodistill-yolov5\n</code></pre>"},{"location":"target_models/yolov5/#quickstart","title":"Quickstart","text":"<pre><code>from autodistill_YOLOv5 import YOLOv5\n\ntarget_model = YOLOv5(\"YOLOv5n.pt\")\n\n# train a model\ntarget_model.train(\"./context_images_labeled/data.yaml\", epochs=200)\n\n# run inference on the new model\npred = target_model.predict(\"./context_images_labeled/train/images/dog-7.jpg\", conf=0.01)\n</code></pre>"},{"location":"target_models/yolov8-instance-segmentation/","title":"YOLOv8 Instance Segmentation","text":""},{"location":"target_models/yolov8-instance-segmentation/#yolov8-instance-segmentation","title":"YOLOv8 Instance Segmentation","text":"<p>YOLOv8 is an open-source computer vision model by Ultralytics, the creators of YOLOv5 that supports object detection, classification, and instance segmentation. You can use <code>autodistill</code> to train a YOLOv8 object detection model on a dataset of labelled images generated by the base models that <code>autodistill</code> supports.</p> <p>This document shows how to train an instance segmentation model using a base model supported by <code>autodiistill</code> and YOLOv8's instance segmentation functionality.</p> <p>View our YOLOv8 page for information on how to train object detectino models.</p>"},{"location":"target_models/yolov8-instance-segmentation/#installation","title":"Installation","text":"<p>To use the YOLOv8 target model, you will need to install the following dependency:</p> <pre><code>pip3 install autodistill-yolov8\n</code></pre>"},{"location":"target_models/yolov8-instance-segmentation/#quickstart","title":"Quickstart","text":"<pre><code>from autodistill_yolov8 import YOLOv8\ntarget_model = YOLOv8(\"yolov8n-seg.pt\")\n\n# train model using images in `context_images_labeled` folder for 200 epochs\ntarget_model.train(\"./context_images_labeled/data.yaml\", epochs=200)\n\n# export weights for future use\nsaved_weights = target_model.export(format=\"onnx\")\n\n# show performance metrics for your model\nmetrics = target_model.val()\n\n# run inference on the new model\npred = target_model.predict(\"./context_images_labeled/train/images/dog-7.jpg\", conf=0.01)\n</code></pre>"},{"location":"target_models/yolov8/","title":"YOLOv8","text":"<p>YOLOv8 is an open-source computer vision model by Ultralytics, the creators of YOLOv5. You can use <code>autodistill</code> to train a YOLOv8 object detection model on a dataset of labelled images generated by the base models that <code>autodistill</code> supports.</p> <p>View our YOLOv8 Instance Segmentation page for information on how to train instance segmentation models.</p>"},{"location":"target_models/yolov8/#installation","title":"Installation","text":"<p>To use the YOLOv8 target model, you will need to install the following dependency:</p> <pre><code>pip3 install autodistill-yolov8\n</code></pre>"},{"location":"target_models/yolov8/#quickstart","title":"Quickstart","text":"<pre><code>from autodistill_yolov8 import YOLOv8\ntarget_model = YOLOv8(\"yolov8n.pt\")\n\n# train a model\ntarget_model.train(\"./context_images_labeled/data.yaml\", epochs=200)\n\n# export weights for future use\nsaved_weights = target_model.export(format=\"onnx\")\n\n# show performance metrics for your model\nmetrics = target_model.val()\n\n# run inference on the new model\npred = target_model.predict(\"./context_images_labeled/train/images/dog-7.jpg\", conf=0.01)\n</code></pre>"}]}